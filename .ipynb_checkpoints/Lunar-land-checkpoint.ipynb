{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import gym\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(8,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed  =12345\n",
    "\n",
    "def run_ddpg(env, num_episodes = 1000, max_steps = 300):\n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.shape[0]\n",
    "\n",
    "    agent = Agent(state_size, action_size, seed)\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    for episode_i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_score = 0\n",
    "        for step in range(max_steps):\n",
    "            action, prob_action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, prob_action, reward, next_state, done)\n",
    "            episode_score += reward\n",
    "            state = next_state\n",
    "            \n",
    "            print(\"\\rstep {} from {}\".format(step, max_steps), end=\"\", flush=True)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            \n",
    "        \n",
    "        score_list.append(episode_score)\n",
    "        score = np.mean(score_list[-100:]) if len(score_list) > 100 else np.mean(score_list)\n",
    "        print(\"\\tspisode:{}\\tescore: {}\".format(episode_i, score))\n",
    "        \n",
    "    \n",
    "    return score, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 135 from 300\tspisode:0\tescore: -262.5507722206812\n",
      "step 81 from 300\tspisode:1\tescore: -120.28596873264543\n",
      "step 83 from 300\tspisode:2\tescore: -315.22219752289055\n",
      "step 51 from 300\tspisode:3\tescore: -346.0711813286032\n",
      "step 62 from 300\tspisode:4\tescore: -393.569181718814\n",
      "step 57 from 300\tspisode:5\tescore: -398.6630186238729\n",
      "step 51 from 300\tspisode:6\tescore: -406.150111125167\n",
      "step 112 from 300\tspisode:7\tescore: -460.584907850071\n",
      "step 146 from 300\tspisode:8\tescore: -501.56667543623684\n",
      "step 123 from 300\tspisode:9\tescore: -513.4858898973523\n",
      "step 278 from 300\tspisode:10\tescore: -697.8184527830053\n",
      "step 95 from 300\tspisode:11\tescore: -678.2175029301948\n",
      "step 112 from 300\tspisode:12\tescore: -683.5973183979788\n",
      "step 250 from 300\tspisode:13\tescore: -778.9017540515532\n",
      "step 99 from 300\tspisode:14\tescore: -767.8324108977608\n",
      "step 47 from 300\tspisode:15\tescore: -743.6250371898818\n",
      "step 75 from 300\tspisode:16\tescore: -744.9103543146523\n",
      "step 48 from 300\tspisode:17\tescore: -726.7744192453877\n",
      "step 48 from 300\tspisode:18\tescore: -704.4456573122974\n",
      "step 73 from 300\tspisode:19\tescore: -705.977343243141\n",
      "step 55 from 300\tspisode:20\tescore: -695.2985666587622\n",
      "step 69 from 300\tspisode:21\tescore: -694.6148212621232\n",
      "step 75 from 300\tspisode:22\tescore: -698.1305355825584\n",
      "step 49 from 300\tspisode:23\tescore: -682.0998687223446\n",
      "step 71 from 300\tspisode:24\tescore: -671.8250309164827\n",
      "step 54 from 300\tspisode:25\tescore: -664.7616629401308\n",
      "step 59 from 300\tspisode:26\tescore: -658.9749651546925\n",
      "step 47 from 300\tspisode:27\tescore: -649.2520594045156\n",
      "step 75 from 300\tspisode:28\tescore: -652.755655385755\n",
      "step 64 from 300\tspisode:29\tescore: -652.34061960807\n",
      "step 68 from 300\tspisode:30\tescore: -651.6878009998637\n",
      "step 71 from 300\tspisode:31\tescore: -645.3363443798854\n",
      "step 49 from 300\tspisode:32\tescore: -636.4794887817304\n",
      "step 76 from 300\tspisode:33\tescore: -639.0620173052712\n",
      "step 49 from 300\tspisode:34\tescore: -631.3541490348853\n",
      "step 56 from 300\tspisode:35\tescore: -629.0144103440698\n",
      "step 103 from 300\tspisode:36\tescore: -628.774713685509\n",
      "step 81 from 300\tspisode:37\tescore: -636.282637186201\n",
      "step 57 from 300\tspisode:38\tescore: -633.5935687528842\n",
      "step 72 from 300\tspisode:39\tescore: -633.5441245090129\n",
      "step 61 from 300\tspisode:40\tescore: -632.6576146545779\n",
      "step 74 from 300\tspisode:41\tescore: -634.1814214653118\n",
      "step 67 from 300\tspisode:42\tescore: -630.9225818623721\n",
      "step 48 from 300\tspisode:43\tescore: -625.82326703624\n",
      "step 48 from 300\tspisode:44\tescore: -620.4978164950857\n",
      "step 56 from 300\tspisode:45\tescore: -618.2395714885865\n",
      "step 54 from 300\tspisode:46\tescore: -613.7139451223366\n",
      "step 61 from 300\tspisode:47\tescore: -612.8302723388447\n",
      "step 53 from 300\tspisode:48\tescore: -610.4950594010506\n",
      "step 65 from 300\tspisode:49\tescore: -609.8276364267257\n",
      "step 57 from 300\tspisode:50\tescore: -608.8644783601047\n",
      "step 51 from 300\tspisode:51\tescore: -605.4122594198948\n",
      "step 51 from 300\tspisode:52\tescore: -596.1363806747124\n",
      "step 75 from 300\tspisode:53\tescore: -587.9163859713445\n",
      "step 50 from 300\tspisode:54\tescore: -579.5033997029246\n",
      "step 52 from 300\tspisode:55\tescore: -571.2078864099637\n",
      "step 67 from 300\tspisode:56\tescore: -563.6216127343845\n",
      "step 57 from 300\tspisode:57\tescore: -555.5695007371927\n",
      "step 60 from 300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a2b2a7d3358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c3d5c1a5af42>\u001b[0m in \u001b[0;36mrun_ddpg\u001b[0;34m(env, num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mepisode_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/drlnd/lunar_land/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/drlnd/lunar_land/ddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# ---------------------------- update actor ---------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, agent = run_ddpg(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd-env",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
